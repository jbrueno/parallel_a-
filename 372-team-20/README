Project Directory Structure and Explanation:

372-team-20/
    README
    updates/
        update1.txt
        update2.txt
    graphs/
        mpi-omp_exec-time.jpg   (graph of the execution time of mpi_threads_astar.c using MPI + OpenMP)
        mpi-omp_speedup.jpg	    (graph of the speedup of mpi_threads_astar.c using MPI + OpenMP)
        mpi_exec-time.jpg	    (graph of the execution time of mpi_threads_astar.c using just MPI)
        mpi_speedup.jpg         (graph of the speedup of mpi_threads_astar.c using just MPI)
    seq/
        astar.c	  (sequential version of the A* search algorithm)
        seq.out   (the output from running the sequential program)
    parallel/
        Makefile             (currently configured to compile the program without openmp, no -fopenmp flag)
        data.txt             (contains data collected from running mpi_threads_astar.c on different numbers of processes)
        mpi_threads_astar.c  (parallel version of A* search algorithm, currently all openmp directives are commented out)
        bridges/             (contains information from all runs on bridges)
            batch/           (contains all of the used batch scripts)
                run1_omp.sh  (runs mpi_threads_astar.c with MPI + OpenMP, 1 full node (1 MPI proc),  28 threads RM partition)
                run2_omp.sh  (runs mpi_threads_astar.c with MPI + OpenMP, 2 full nodes(2 MPI proc), 56 threads RM partition)
                run3_omp.sh  (runs mpi_threads_astar.c with MPI + OpenMP, 3 full nodes(3 MPI proc), 84 threads RM partition)
                run2.sh      (runs mpi_threads_astar.c with MPI only, 1 node, 2 procs, RM-shared partition)
                run4.sh	     (runs mpi_threads_astar.c with MPI only, 1 node, 4 procs, RM-shared partition)
                run8.sh      (runs mpi_threads_astar.c with MPI only, 1 node, 8 procs, RM-shared partition)
                run12.sh     (runs mpi_threads_astar.c with MPI only, 1 node, 12 procs, RM-shared partition)
                run16.sh     (runs mpi_threads_astar.c with MPI only, 1 node, 16 procs, RM-shared partition)
                run20.sh     (runs mpi_threads_astar.c with MPI only, 1 node, 20 procs, RM-shared partition)
                run24.sh     (runs mpi_threads_astar.c with MPI only, 1 node, 24 procs, RM-shared partition)
                run28.sh     (runs mpi_threads_astar.c with MPI only, 1 node, 28 procs, RM)
                run56.sh     (runs mpi_threads_astar.c with MPI only, 1 node, 54 procs, RM)
                run84.sh     (runs mpi_threads_astar.c with MPI only, 1 node, 86 procs, RM)
            out/
                1proc_omp.out   (output from run1_omp.sh)
                2procs_omp.out  (output from run2_omp.sh)
                3procs_omp.out  (output from run3_omp.sh)
                2procs.out      (output from run2.sh)
                4procs.out      (output from run4.sh)
                8procs.out      (output from run8.sh)
                12procs.out     (output from run12.sh)
                16procs.out     (output from run16.sh)
                20procs.out     (output from run20.sh)
                24procs.out     (output from run24.sh)
                28procs.out     (output from run28.sh)
                56procs.out     (output from run56.sh)
                84procs.out     (output from run84.sh)
                	
                

Project Overview:

Objective: solve a 15-puzzle using parallel A* search (used MPI only and MPI + OpenMP)

A 15-puzzle is a 4x4 board comprised of numbers from 1-15 and a blank space.  The blank space
on the board allows for the numbered tiles to be shifted around.  In my code, I used the
number 0 to represent the blank space to allow the other numbers to move.  A solution to the
puzzle is considered to be the state in which all of the numbered tiles are arranged in order
starting from the top left to the bottom right.  In my case I used the top left tile on the
board to be the location of the 0 or blank tile for the GOAL state, but other solutions may 
have the blank tile located at the bottom right of the board.

(when I say explored I mean the boards children have been checked and the board is moved to the closed list)

An A* search algorithm is an informed search meaning it uses a heuristic function to aid
its execution.  A heuristic basically informs the algorithm of how close a node is
to the solution.  I used the Manhattan distance for all of the tests run on my code.
Inside of my A* search, I start by creating an open list and a closed list.  The open
list will contain all of the boards that have not yet been explored and the closed list
will contain all of the boards that have been explored.  To start the algorithm off, the 
initial board is added to the open list.  The algorithm then enters a big while loop that
executes while the open list is not empty. At the beginning of each loop, it finds the 
board with the lowest heuristic value, explores its children, and moves it to the closed 
list.  If the child it is examining is already on the closed list, it moves to the next 
child.  If the child board is already on the open list it compares them and only keeps
the better of the two.  If it is neither on the open list nor the closed list, it is added
to the open list for later exploration.  This continues until the board being explored
is equal to the GOAL board.
In order to parallelize my algorithm, I decided that each MPI process will receive a 
different portion of the open list to check.  The parallelization begins once the open
list is big enough for each process to check at least 2 boards.  
A big challenge arose in this parallel version which is that it will continue to run unless 
all of the MPI processes receive an alert of some kind once one of the procs has found a solution.
I was not able to solve this issue and I had to manually examine which process would find
the solution first. Everytime a different number of processes was run on the program,
the value of the int 'first_proc_finished' (found at the top of the code between two lines 
of asterisks) had to be changed to the proc number that will find the solution first.
In the data.txt file at the end of each line describing the parallel run is the number
of the proc that found the solution (ex: 'proc 11 found solution').

The input board I used in the sequential version and all of the parallel tests (TEST) can be 
solved in a MINIMUM of 80 moves.  80 moves is the highest number of moves required to solve 
any solvable 15-puzzle instance.


How to compile run the code:

Sequential: astar.c
    I compiled this every time using the following:
        gcc -o astar.exec astar.c
    run with ./astar.exec

    The program will run for about 15 seconds and print out whether or not
    a solution was found, the sequence of the solution, the number of moves
    it took to find the solution (path cost), the initial board and final
    board, and the total time the algorithm took.

Parallel: mpi_threads_astar.c

	**********************************************************************************************************************
	VALUE OF 'first_proc_finished'(LINE 20) MUST BE CHANGED BEFORE COMPILING FOR EACH RUN WITH A DIFFERENT # OF MPI PROCS
	------- the value to change it to can be found at the end of this file or at the end of each line in data.txt --------
	**********************************************************************************************************************

	I used the Makefile to compile this program each time.  It must be
	updated depending on whether or not openmp is being used.


    for MPI only compile with the following

        mpicc -o mpi_threads_astar.exec mpi_threads_astar.c

    run on Bridges with any of the following batch scripts:
        run2.sh, run4.sh, run8.sh, run12.sh, run16.sh, run20.sh,
        run24.sh, run28.sh, run56.sh, or run84.sh
    (on my 2 core macbook, I ran the program with: mpirun -np 2 ./mpi_threads_astar.exec)


    for MPI + OpenMP compile with the following:

    mpicc -fopenmp -o mpi_threads_astar.exec mpi_threads_astar.c

	TO RUN: UNCOMMENT LINES 8, 286, 377, 405, and 454
    run on bridges with batch scripts 1proc_omp.out, 2procs_omp.out, or 3procs_omp.out




VALUE OF 'first_proc_finished' FOR EACH OF THE FOLLOWING NUMBER OF MPI PROCS

1 PROC:    0
2 PROCS:   1
3 PROCS:   2
4 PROCS:   3
8 PROCS:   7
12 PROCS:  11
16 PROCS:  14
20 PROCS:  17
24 PROCS:  23
28 PROCS:  27
56 PROCS:  54
84 PROCS:  82
